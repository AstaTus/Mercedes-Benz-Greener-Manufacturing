{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DevelopKit\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "D:\\DevelopKit\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/original/train.csv', index_col=0)\n",
    "test_df = pd.read_csv('../data/original/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame(train_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.20609583])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skew(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_df = np.log1p(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.drop(['y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def munge(df):\n",
    "    all_df = pd.DataFrame(df.values, index=df.index, columns=df.columns, copy=True)\n",
    "    all_df.drop(['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    #删除取值相同的特征\n",
    "    all_df.drop(['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290' ,'X293' ,'X297', 'X330' ,'X347'], axis=1, inplace=True)\n",
    "    \n",
    "    #构造新特征\n",
    "    all_df['parts'] = all_df.sum(axis=1)\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "munged_train_df = munge(train_df)\n",
    "munged_test_df = munge(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DevelopKit\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "D:\\DevelopKit\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "D:\\DevelopKit\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(munged_train_df['parts'])\n",
    "\n",
    "scaled = scaler.transform(munged_train_df['parts'])\n",
    "munged_train_df['parts'] = scaled\n",
    "\n",
    "scaled = scaler.transform(munged_test_df['parts'])\n",
    "munged_test_df['parts'] = scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert categorical features using one-hot encoding.\n",
    "def onehot(onehot_df, df, column_name, fill_na):\n",
    "    onehot_df[column_name] = df[column_name]\n",
    "    if fill_na is not None:\n",
    "        onehot_df[column_name].fillna(fill_na, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(onehot_df[column_name], prefix = column_name)\n",
    "    \n",
    "    onehot_df = onehot_df.join(dummies)\n",
    "    onehot_df = onehot_df.drop([column_name], axis=1)\n",
    "    return onehot_df\n",
    "\n",
    "def munge_onehot(df):\n",
    "    onehot_df = pd.DataFrame(index = df.index)\n",
    "\n",
    "    onehot_df = onehot(onehot_df, df, \"X0\", None)\n",
    "    onehot_df = onehot(onehot_df, df, \"X1\", None)\n",
    "    onehot_df = onehot(onehot_df, df, \"X2\", None)\n",
    "    onehot_df = onehot(onehot_df, df, \"X3\", None)\n",
    "    onehot_df = onehot(onehot_df, df, \"X4\", None)\n",
    "    onehot_df = onehot(onehot_df, df, \"X5\", None)\n",
    "    onehot_df = onehot(onehot_df, df, \"X6\", None)\n",
    "    onehot_df = onehot(onehot_df, df, \"X8\", None)\n",
    "    \n",
    "    return onehot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onehot_df = munge_onehot(train_df)\n",
    "munged_train_df = munged_train_df.join(onehot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onehot_df = munge_onehot(test_df)\n",
    "munged_test_df = munged_test_df.join(onehot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X0_ae',\n",
       " 'X0_ag',\n",
       " 'X0_an',\n",
       " 'X0_av',\n",
       " 'X0_bb',\n",
       " 'X0_p',\n",
       " 'X2_ab',\n",
       " 'X2_ad',\n",
       " 'X2_aj',\n",
       " 'X2_ax',\n",
       " 'X2_u',\n",
       " 'X2_w',\n",
       " 'X5_a',\n",
       " 'X5_b',\n",
       " 'X5_t',\n",
       " 'X5_z'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(munged_test_df) - set(munged_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#删除test中有的  而train中没有的\n",
    "munged_test_df.drop(['X0_ae', 'X0_ag', 'X0_an', 'X0_av', 'X0_bb', 'X0_p',\n",
    "                     'X2_ab', 'X2_ad', 'X2_aj', 'X2_ax', 'X2_u', 'X2_w', 'X5_a', 'X5_b', 'X5_t', 'X5_z'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X0_aa',\n",
       " 'X0_ab',\n",
       " 'X0_ac',\n",
       " 'X0_q',\n",
       " 'X2_aa',\n",
       " 'X2_ar',\n",
       " 'X2_c',\n",
       " 'X2_l',\n",
       " 'X2_o',\n",
       " 'X5_u'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(munged_train_df) - set(munged_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#删除train中有的  而test中没有的\n",
    "munged_train_df.drop(['X0_aa', 'X0_ab', 'X0_ac', 'X0_q', 'X2_aa', 'X2_ar', 'X2_c', 'X2_l', 'X2_o', 'X5_u'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#删除一些占比非常不平衡的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4153"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "munged_train_df['X10'].value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X10 p1 = 0.986695 p2 = 0.013305\n",
      "X15 p1 = 0.999525 p2 = 0.000475\n",
      "X16 p1 = 0.997387 p2 = 0.002613\n",
      "X17 p1 = 0.992397 p2 = 0.007603\n",
      "X18 p1 = 0.992160 p2 = 0.007840\n",
      "X21 p1 = 0.997387 p2 = 0.002613\n",
      "X23 p1 = 0.979330 p2 = 0.020670\n",
      "X24 p1 = 0.998099 p2 = 0.001901\n",
      "X26 p1 = 0.995011 p2 = 0.004989\n",
      "X28 p1 = 0.967451 p2 = 0.032549\n",
      "X29 p1 = 0.956997 p2 = 0.043003\n",
      "X30 p1 = 0.995486 p2 = 0.004514\n",
      "X32 p1 = 0.988833 p2 = 0.011167\n",
      "X33 p1 = 0.999762 p2 = 0.000238\n",
      "X34 p1 = 0.994536 p2 = 0.005464\n",
      "X36 p1 = 0.995486 p2 = 0.004514\n",
      "X38 p1 = 0.966738 p2 = 0.033262\n",
      "X39 p1 = 0.999762 p2 = 0.000238\n",
      "X40 p1 = 0.999287 p2 = 0.000713\n",
      "X41 p1 = 0.988596 p2 = 0.011404\n",
      "X42 p1 = 0.999762 p2 = 0.000238\n",
      "X44 p1 = 0.988596 p2 = 0.011404\n",
      "X47 p1 = 0.987170 p2 = 0.012830\n",
      "X48 p1 = 0.977667 p2 = 0.022333\n",
      "X52 p1 = 0.957710 p2 = 0.042290\n",
      "X53 p1 = 0.993110 p2 = 0.006890\n",
      "X54 p1 = 0.956522 p2 = 0.043478\n",
      "X55 p1 = 0.994773 p2 = 0.005227\n",
      "X56 p1 = 0.978855 p2 = 0.021145\n",
      "X57 p1 = 0.986695 p2 = 0.013305\n",
      "X59 p1 = 0.999287 p2 = 0.000713\n",
      "X60 p1 = 0.998574 p2 = 0.001426\n",
      "X61 p1 = 0.046092 p2 = 0.953908\n",
      "X62 p1 = 0.994060 p2 = 0.005940\n",
      "X63 p1 = 0.988596 p2 = 0.011404\n",
      "X65 p1 = 0.997862 p2 = 0.002138\n",
      "X66 p1 = 0.972915 p2 = 0.027085\n",
      "X67 p1 = 0.998099 p2 = 0.001901\n",
      "X69 p1 = 0.970064 p2 = 0.029936\n",
      "X73 p1 = 0.980043 p2 = 0.019957\n",
      "X74 p1 = 0.000713 p2 = 0.999287\n",
      "X75 p1 = 0.963887 p2 = 0.036113\n",
      "X76 p1 = 0.956522 p2 = 0.043478\n",
      "X77 p1 = 0.987408 p2 = 0.012592\n",
      "X78 p1 = 0.994298 p2 = 0.005702\n",
      "X79 p1 = 0.974816 p2 = 0.025184\n",
      "X82 p1 = 0.982894 p2 = 0.017106\n",
      "X83 p1 = 0.998812 p2 = 0.001188\n",
      "X86 p1 = 0.998574 p2 = 0.001426\n",
      "X87 p1 = 0.999050 p2 = 0.000950\n",
      "X88 p1 = 0.992872 p2 = 0.007128\n",
      "X89 p1 = 0.999287 p2 = 0.000713\n",
      "X90 p1 = 0.992635 p2 = 0.007365\n",
      "X91 p1 = 0.998337 p2 = 0.001663\n",
      "X92 p1 = 0.999050 p2 = 0.000950\n",
      "X94 p1 = 0.992635 p2 = 0.007365\n",
      "X95 p1 = 0.999762 p2 = 0.000238\n",
      "X97 p1 = 0.995723 p2 = 0.004277\n",
      "X99 p1 = 0.991447 p2 = 0.008553\n",
      "X102 p1 = 0.993110 p2 = 0.006890\n",
      "X104 p1 = 0.998099 p2 = 0.001901\n",
      "X105 p1 = 0.997624 p2 = 0.002376\n",
      "X106 p1 = 0.986933 p2 = 0.013067\n",
      "X108 p1 = 0.985270 p2 = 0.014730\n",
      "X109 p1 = 0.959610 p2 = 0.040390\n",
      "X110 p1 = 0.999050 p2 = 0.000950\n",
      "X111 p1 = 0.025184 p2 = 0.974816\n",
      "X112 p1 = 0.997149 p2 = 0.002851\n",
      "X113 p1 = 0.977667 p2 = 0.022333\n",
      "X117 p1 = 0.950820 p2 = 0.049180\n",
      "X120 p1 = 0.042290 p2 = 0.957710\n",
      "X122 p1 = 0.992872 p2 = 0.007128\n",
      "X123 p1 = 0.997387 p2 = 0.002613\n",
      "X124 p1 = 0.999525 p2 = 0.000475\n",
      "X125 p1 = 0.996911 p2 = 0.003089\n",
      "X126 p1 = 0.961036 p2 = 0.038964\n",
      "X128 p1 = 0.041578 p2 = 0.958422\n",
      "X130 p1 = 0.958422 p2 = 0.041578\n",
      "X131 p1 = 0.973390 p2 = 0.026610\n",
      "X134 p1 = 0.977667 p2 = 0.022333\n",
      "X135 p1 = 0.972915 p2 = 0.027085\n",
      "X136 p1 = 0.043478 p2 = 0.956522\n",
      "X138 p1 = 0.959135 p2 = 0.040865\n",
      "X140 p1 = 0.959610 p2 = 0.040390\n",
      "X141 p1 = 0.985745 p2 = 0.014255\n",
      "X143 p1 = 0.961749 p2 = 0.038251\n",
      "X145 p1 = 0.998574 p2 = 0.001426\n",
      "X146 p1 = 0.959135 p2 = 0.040865\n",
      "X147 p1 = 0.977667 p2 = 0.022333\n",
      "X148 p1 = 0.955096 p2 = 0.044904\n",
      "X152 p1 = 0.967688 p2 = 0.032312\n",
      "X153 p1 = 0.999287 p2 = 0.000713\n",
      "X159 p1 = 0.986458 p2 = 0.013542\n",
      "X160 p1 = 0.998812 p2 = 0.001188\n",
      "X162 p1 = 0.959135 p2 = 0.040865\n",
      "X165 p1 = 0.995486 p2 = 0.004514\n",
      "X166 p1 = 0.966738 p2 = 0.033262\n",
      "X167 p1 = 0.999050 p2 = 0.000950\n",
      "X169 p1 = 0.993348 p2 = 0.006652\n",
      "X170 p1 = 0.975766 p2 = 0.024234\n",
      "X172 p1 = 0.994060 p2 = 0.005940\n",
      "X173 p1 = 0.990259 p2 = 0.009741\n",
      "X174 p1 = 0.982656 p2 = 0.017344\n",
      "X175 p1 = 0.977667 p2 = 0.022333\n",
      "X176 p1 = 0.982894 p2 = 0.017106\n",
      "X179 p1 = 0.952008 p2 = 0.047992\n",
      "X183 p1 = 0.995961 p2 = 0.004039\n",
      "X184 p1 = 0.998574 p2 = 0.001426\n",
      "X185 p1 = 0.981231 p2 = 0.018769\n",
      "X190 p1 = 0.999762 p2 = 0.000238\n",
      "X192 p1 = 0.997624 p2 = 0.002376\n",
      "X195 p1 = 0.988358 p2 = 0.011642\n",
      "X196 p1 = 0.989784 p2 = 0.010216\n",
      "X197 p1 = 0.967688 p2 = 0.032312\n",
      "X198 p1 = 0.976954 p2 = 0.023046\n",
      "X199 p1 = 0.997149 p2 = 0.002851\n",
      "X200 p1 = 0.993348 p2 = 0.006652\n",
      "X203 p1 = 0.983131 p2 = 0.016869\n",
      "X204 p1 = 0.999762 p2 = 0.000238\n",
      "X205 p1 = 0.000238 p2 = 0.999762\n",
      "X206 p1 = 0.980756 p2 = 0.019244\n",
      "X207 p1 = 0.999762 p2 = 0.000238\n",
      "X210 p1 = 0.999762 p2 = 0.000238\n",
      "X211 p1 = 0.985032 p2 = 0.014968\n",
      "X212 p1 = 0.994536 p2 = 0.005464\n",
      "X213 p1 = 0.998099 p2 = 0.001901\n",
      "X214 p1 = 0.993110 p2 = 0.006890\n",
      "X216 p1 = 0.994060 p2 = 0.005940\n",
      "X217 p1 = 0.992635 p2 = 0.007365\n",
      "X221 p1 = 0.991922 p2 = 0.008078\n",
      "X222 p1 = 0.977667 p2 = 0.022333\n",
      "X226 p1 = 0.967688 p2 = 0.032312\n",
      "X227 p1 = 0.996911 p2 = 0.003089\n",
      "X228 p1 = 0.961036 p2 = 0.038964\n",
      "X229 p1 = 0.039914 p2 = 0.960086\n",
      "X230 p1 = 0.994773 p2 = 0.005227\n",
      "X231 p1 = 0.983844 p2 = 0.016156\n",
      "X232 p1 = 0.956997 p2 = 0.043003\n",
      "X236 p1 = 0.999525 p2 = 0.000475\n",
      "X237 p1 = 0.993348 p2 = 0.006652\n",
      "X239 p1 = 0.993110 p2 = 0.006890\n",
      "X240 p1 = 0.997149 p2 = 0.002851\n",
      "X242 p1 = 0.992635 p2 = 0.007365\n",
      "X243 p1 = 0.992872 p2 = 0.007128\n",
      "X245 p1 = 0.999287 p2 = 0.000713\n",
      "X248 p1 = 0.998574 p2 = 0.001426\n",
      "X249 p1 = 0.992397 p2 = 0.007603\n",
      "X252 p1 = 0.999287 p2 = 0.000713\n",
      "X253 p1 = 0.998574 p2 = 0.001426\n",
      "X254 p1 = 0.994773 p2 = 0.005227\n",
      "X255 p1 = 0.980518 p2 = 0.019482\n",
      "X257 p1 = 0.999762 p2 = 0.000238\n",
      "X258 p1 = 0.997624 p2 = 0.002376\n",
      "X259 p1 = 0.999762 p2 = 0.000238\n",
      "X260 p1 = 0.999762 p2 = 0.000238\n",
      "X262 p1 = 0.998574 p2 = 0.001426\n",
      "X263 p1 = 0.043003 p2 = 0.956997\n",
      "X264 p1 = 0.960561 p2 = 0.039439\n",
      "X266 p1 = 0.998574 p2 = 0.001426\n",
      "X267 p1 = 0.990972 p2 = 0.009028\n",
      "X269 p1 = 0.999525 p2 = 0.000475\n",
      "X270 p1 = 0.999762 p2 = 0.000238\n",
      "X271 p1 = 0.997862 p2 = 0.002138\n",
      "X272 p1 = 0.962461 p2 = 0.037539\n",
      "X274 p1 = 0.990021 p2 = 0.009979\n",
      "X276 p1 = 0.961511 p2 = 0.038489\n",
      "X277 p1 = 0.998574 p2 = 0.001426\n",
      "X278 p1 = 0.999525 p2 = 0.000475\n",
      "X279 p1 = 0.956997 p2 = 0.043003\n",
      "X280 p1 = 0.999762 p2 = 0.000238\n",
      "X281 p1 = 0.997387 p2 = 0.002613\n",
      "X282 p1 = 0.995961 p2 = 0.004039\n",
      "X284 p1 = 0.958898 p2 = 0.041102\n",
      "X287 p1 = 0.984082 p2 = 0.015918\n",
      "X288 p1 = 0.999762 p2 = 0.000238\n",
      "X291 p1 = 0.989546 p2 = 0.010454\n",
      "X292 p1 = 0.990972 p2 = 0.009028\n",
      "X295 p1 = 0.999762 p2 = 0.000238\n",
      "X296 p1 = 0.999762 p2 = 0.000238\n",
      "X298 p1 = 0.995486 p2 = 0.004514\n",
      "X299 p1 = 0.995486 p2 = 0.004514\n",
      "X301 p1 = 0.953196 p2 = 0.046804\n",
      "X302 p1 = 0.988596 p2 = 0.011404\n",
      "X305 p1 = 0.986695 p2 = 0.013305\n",
      "X306 p1 = 0.956284 p2 = 0.043716\n",
      "X307 p1 = 0.997862 p2 = 0.002138\n",
      "X308 p1 = 0.990497 p2 = 0.009503\n",
      "X309 p1 = 0.992872 p2 = 0.007128\n",
      "X310 p1 = 0.997387 p2 = 0.002613\n",
      "X312 p1 = 0.995723 p2 = 0.004277\n",
      "X315 p1 = 0.971252 p2 = 0.028748\n",
      "X317 p1 = 0.992397 p2 = 0.007603\n",
      "X318 p1 = 0.999287 p2 = 0.000713\n",
      "X319 p1 = 0.999525 p2 = 0.000475\n",
      "X320 p1 = 0.992872 p2 = 0.007128\n",
      "X322 p1 = 0.978142 p2 = 0.021858\n",
      "X323 p1 = 0.990734 p2 = 0.009266\n",
      "X325 p1 = 0.994298 p2 = 0.005702\n",
      "X326 p1 = 0.967688 p2 = 0.032312\n",
      "X328 p1 = 0.959848 p2 = 0.040152\n",
      "X332 p1 = 0.999287 p2 = 0.000713\n",
      "X333 p1 = 0.976004 p2 = 0.023996\n",
      "X335 p1 = 0.996436 p2 = 0.003564\n",
      "X338 p1 = 0.993110 p2 = 0.006890\n",
      "X339 p1 = 0.999762 p2 = 0.000238\n",
      "X340 p1 = 0.977667 p2 = 0.022333\n",
      "X341 p1 = 0.991922 p2 = 0.008078\n",
      "X342 p1 = 0.977667 p2 = 0.022333\n",
      "X344 p1 = 0.991447 p2 = 0.008553\n",
      "X345 p1 = 0.977667 p2 = 0.022333\n",
      "X346 p1 = 0.952483 p2 = 0.047517\n",
      "X349 p1 = 0.955096 p2 = 0.044904\n",
      "X353 p1 = 0.997862 p2 = 0.002138\n",
      "X357 p1 = 0.998812 p2 = 0.001188\n",
      "X359 p1 = 0.968163 p2 = 0.031837\n",
      "X361 p1 = 0.033975 p2 = 0.966025\n",
      "X364 p1 = 0.997149 p2 = 0.002851\n",
      "X365 p1 = 0.997149 p2 = 0.002851\n",
      "X366 p1 = 0.998812 p2 = 0.001188\n",
      "X369 p1 = 0.999525 p2 = 0.000475\n",
      "X370 p1 = 0.993348 p2 = 0.006652\n",
      "X371 p1 = 0.985745 p2 = 0.014255\n",
      "X372 p1 = 0.999525 p2 = 0.000475\n",
      "X373 p1 = 0.980756 p2 = 0.019244\n",
      "X378 p1 = 0.979330 p2 = 0.020670\n",
      "X379 p1 = 0.990497 p2 = 0.009503\n",
      "X380 p1 = 0.991922 p2 = 0.008078\n",
      "X382 p1 = 0.992397 p2 = 0.007603\n",
      "X383 p1 = 0.998337 p2 = 0.001663\n",
      "X384 p1 = 0.999525 p2 = 0.000475\n",
      "X385 p1 = 0.998574 p2 = 0.001426\n",
      "X0_a p1 = 0.995011 p2 = 0.004989\n",
      "X0_ad p1 = 0.996674 p2 = 0.003326\n",
      "X0_af p1 = 0.991684 p2 = 0.008316\n",
      "X0_ai p1 = 0.991922 p2 = 0.008078\n",
      "X0_aj p1 = 0.964124 p2 = 0.035876\n",
      "X0_al p1 = 0.984082 p2 = 0.015918\n",
      "X0_am p1 = 0.995723 p2 = 0.004277\n",
      "X0_ao p1 = 0.999050 p2 = 0.000950\n",
      "X0_ap p1 = 0.975529 p2 = 0.024471\n",
      "X0_aq p1 = 0.995723 p2 = 0.004277\n",
      "X0_as p1 = 0.997624 p2 = 0.002376\n",
      "X0_at p1 = 0.994060 p2 = 0.005940\n",
      "X0_au p1 = 0.997387 p2 = 0.002613\n",
      "X0_aw p1 = 0.996199 p2 = 0.003801\n",
      "X0_ax p1 = 0.995486 p2 = 0.004514\n",
      "X0_az p1 = 0.958422 p2 = 0.041578\n",
      "X0_b p1 = 0.997387 p2 = 0.002613\n",
      "X0_ba p1 = 0.993585 p2 = 0.006415\n",
      "X0_bc p1 = 0.998574 p2 = 0.001426\n",
      "X0_c p1 = 0.999287 p2 = 0.000713\n",
      "X0_d p1 = 0.982656 p2 = 0.017344\n",
      "X0_e p1 = 0.992397 p2 = 0.007603\n",
      "X0_g p1 = 0.999762 p2 = 0.000238\n",
      "X0_h p1 = 0.982181 p2 = 0.017819\n",
      "X0_i p1 = 0.995723 p2 = 0.004277\n",
      "X0_j p1 = 0.956997 p2 = 0.043003\n",
      "X0_k p1 = 0.997387 p2 = 0.002613\n",
      "X0_l p1 = 0.996199 p2 = 0.003801\n",
      "X0_m p1 = 0.991922 p2 = 0.008078\n",
      "X0_n p1 = 0.953671 p2 = 0.046329\n",
      "X0_r p1 = 0.997624 p2 = 0.002376\n",
      "X0_s p1 = 0.974816 p2 = 0.025184\n",
      "X0_u p1 = 0.995961 p2 = 0.004039\n",
      "X0_v p1 = 0.991447 p2 = 0.008553\n",
      "X0_w p1 = 0.956759 p2 = 0.043241\n",
      "X1_a p1 = 0.966025 p2 = 0.033975\n",
      "X1_ab p1 = 0.999287 p2 = 0.000713\n",
      "X1_c p1 = 0.971252 p2 = 0.028748\n",
      "X1_d p1 = 0.999287 p2 = 0.000713\n",
      "X1_e p1 = 0.992160 p2 = 0.007840\n",
      "X1_f p1 = 0.994536 p2 = 0.005464\n",
      "X1_g p1 = 0.998574 p2 = 0.001426\n",
      "X1_h p1 = 0.993110 p2 = 0.006890\n",
      "X1_i p1 = 0.951770 p2 = 0.048230\n",
      "X1_j p1 = 0.994773 p2 = 0.005227\n",
      "X1_k p1 = 0.995961 p2 = 0.004039\n",
      "X1_m p1 = 0.992397 p2 = 0.007603\n",
      "X1_n p1 = 0.995486 p2 = 0.004514\n",
      "X1_o p1 = 0.980518 p2 = 0.019482\n",
      "X1_p p1 = 0.997862 p2 = 0.002138\n",
      "X1_q p1 = 0.999287 p2 = 0.000713\n",
      "X1_t p1 = 0.992635 p2 = 0.007365\n",
      "X1_u p1 = 0.991209 p2 = 0.008791\n",
      "X1_w p1 = 0.987646 p2 = 0.012354\n",
      "X1_y p1 = 0.994536 p2 = 0.005464\n",
      "X1_z p1 = 0.989071 p2 = 0.010929\n",
      "X2_a p1 = 0.988833 p2 = 0.011167\n",
      "X2_ac p1 = 0.996911 p2 = 0.003089\n",
      "X2_af p1 = 0.999762 p2 = 0.000238\n",
      "X2_ag p1 = 0.995486 p2 = 0.004514\n",
      "X2_ah p1 = 0.999050 p2 = 0.000950\n",
      "X2_al p1 = 0.998812 p2 = 0.001188\n",
      "X2_am p1 = 0.999762 p2 = 0.000238\n",
      "X2_an p1 = 0.998812 p2 = 0.001188\n",
      "X2_ao p1 = 0.995248 p2 = 0.004752\n",
      "X2_ap p1 = 0.997387 p2 = 0.002613\n",
      "X2_aq p1 = 0.985032 p2 = 0.014968\n",
      "X2_at p1 = 0.998574 p2 = 0.001426\n",
      "X2_au p1 = 0.999287 p2 = 0.000713\n",
      "X2_av p1 = 0.999050 p2 = 0.000950\n",
      "X2_aw p1 = 0.998099 p2 = 0.001901\n",
      "X2_ay p1 = 0.987170 p2 = 0.012830\n",
      "X2_b p1 = 0.995011 p2 = 0.004989\n",
      "X2_d p1 = 0.995723 p2 = 0.004277\n",
      "X2_e p1 = 0.980756 p2 = 0.019244\n",
      "X2_f p1 = 0.979330 p2 = 0.020670\n",
      "X2_g p1 = 0.997149 p2 = 0.002851\n",
      "X2_h p1 = 0.998574 p2 = 0.001426\n",
      "X2_i p1 = 0.994060 p2 = 0.005940\n",
      "X2_j p1 = 0.999762 p2 = 0.000238\n",
      "X2_k p1 = 0.994060 p2 = 0.005940\n",
      "X2_n p1 = 0.967451 p2 = 0.032549\n",
      "X2_p p1 = 0.999050 p2 = 0.000950\n",
      "X2_q p1 = 0.998812 p2 = 0.001188\n",
      "X2_r p1 = 0.963649 p2 = 0.036351\n",
      "X2_s p1 = 0.977667 p2 = 0.022333\n",
      "X2_t p1 = 0.993110 p2 = 0.006890\n",
      "X2_x p1 = 0.997624 p2 = 0.002376\n",
      "X2_y p1 = 0.997387 p2 = 0.002613\n",
      "X2_z p1 = 0.995486 p2 = 0.004514\n",
      "X3_b p1 = 0.986458 p2 = 0.013542\n",
      "X3_e p1 = 0.961273 p2 = 0.038727\n",
      "X4_a p1 = 0.999525 p2 = 0.000475\n",
      "X4_b p1 = 0.999762 p2 = 0.000238\n",
      "X4_c p1 = 0.999762 p2 = 0.000238\n",
      "X4_d p1 = 0.000950 p2 = 0.999050\n",
      "X5_aa p1 = 0.973390 p2 = 0.026610\n",
      "X5_ab p1 = 0.953196 p2 = 0.046804\n",
      "X5_ac p1 = 0.952483 p2 = 0.047517\n",
      "X5_ad p1 = 0.956047 p2 = 0.043953\n",
      "X5_ae p1 = 0.951295 p2 = 0.048705\n",
      "X5_af p1 = 0.955334 p2 = 0.044666\n",
      "X5_ag p1 = 0.951532 p2 = 0.048468\n",
      "X5_ah p1 = 0.976954 p2 = 0.023046\n",
      "X5_c p1 = 0.968876 p2 = 0.031124\n",
      "X5_f p1 = 0.998337 p2 = 0.001663\n",
      "X5_g p1 = 0.999762 p2 = 0.000238\n",
      "X5_h p1 = 0.999762 p2 = 0.000238\n",
      "X5_i p1 = 0.950820 p2 = 0.049180\n",
      "X5_j p1 = 0.970302 p2 = 0.029698\n",
      "X5_k p1 = 0.957947 p2 = 0.042053\n",
      "X5_l p1 = 0.953671 p2 = 0.046329\n",
      "X5_m p1 = 0.950582 p2 = 0.049418\n",
      "X5_o p1 = 0.995248 p2 = 0.004752\n",
      "X5_p p1 = 0.950582 p2 = 0.049418\n",
      "X5_x p1 = 0.999525 p2 = 0.000475\n",
      "X5_y p1 = 0.999762 p2 = 0.000238\n",
      "X6_a p1 = 0.951057 p2 = 0.048943\n",
      "X6_b p1 = 0.993348 p2 = 0.006652\n",
      "X6_c p1 = 0.990972 p2 = 0.009028\n",
      "X6_e p1 = 0.997149 p2 = 0.002851\n",
      "X6_f p1 = 0.995248 p2 = 0.004752\n",
      "X6_h p1 = 0.954859 p2 = 0.045141\n",
      "X6_k p1 = 0.989784 p2 = 0.010216\n",
      "X8_a p1 = 0.950107 p2 = 0.049893\n",
      "X8_b p1 = 0.954859 p2 = 0.045141\n",
      "X8_c p1 = 0.976241 p2 = 0.023759\n",
      "X8_d p1 = 0.975529 p2 = 0.024471\n",
      "X8_g p1 = 0.969114 p2 = 0.030886\n",
      "X8_h p1 = 0.972202 p2 = 0.027798\n",
      "X8_k p1 = 0.958185 p2 = 0.041815\n",
      "X8_l p1 = 0.976004 p2 = 0.023996\n",
      "X8_m p1 = 0.963174 p2 = 0.036826\n",
      "X8_o p1 = 0.961273 p2 = 0.038727\n",
      "X8_p p1 = 0.976241 p2 = 0.023759\n",
      "X8_q p1 = 0.972202 p2 = 0.027798\n",
      "X8_t p1 = 0.971727 p2 = 0.028273\n",
      "X8_u p1 = 0.971727 p2 = 0.028273\n",
      "X8_v p1 = 0.953908 p2 = 0.046092\n",
      "X8_w p1 = 0.953433 p2 = 0.046567\n",
      "X8_x p1 = 0.975053 p2 = 0.024947\n",
      "X8_y p1 = 0.972440 p2 = 0.027560\n"
     ]
    }
   ],
   "source": [
    "s = munged_train_df.shape[0]\n",
    "drop_names = []\n",
    "for c in munged_train_df.drop(['parts'], axis=1).columns:\n",
    "    a = munged_train_df[c].value_counts()[0] / s\n",
    "    b = munged_train_df[c].value_counts()[1] / s\n",
    "    if (a < 0.05 or b < 0.05):\n",
    "        print('%s p1 = %f p2 = %f'%(c, a, b))\n",
    "        drop_names.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(drop_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "munged_train_df.drop(drop_names, axis=1, inplace=True)\n",
    "munged_test_df.drop(drop_names, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_comp = 10\n",
    "\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "pca2_results_train = pca.fit_transform(munged_train_df)\n",
    "pca2_results_test = pca.transform(munged_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica2_results_train = ica.fit_transform(munged_train_df)\n",
    "ica2_results_test = ica.transform(munged_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp+1):\n",
    "    munged_train_df['pca_' + str(i)] = pca2_results_train[:,i-1]\n",
    "    munged_test_df['pca_' + str(i)] = pca2_results_test[:, i-1]\n",
    "    \n",
    "    munged_train_df['ica_' + str(i)] = ica2_results_train[:,i-1]\n",
    "    munged_test_df['ica_' + str(i)] = ica2_results_test[:, i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "munged_train_df.to_csv('../data/offline/train.csv')\n",
    "munged_test_df.to_csv('../data/offline/test.csv')\n",
    "label_df.to_csv('../data/offline/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, feature_names, X_train, y_train, X_test, y_test, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X_train, y_train)\n",
    "\n",
    "    #Predict training set:\n",
    "    y_predictions = alg.predict(X_test)\n",
    "#     dtrain_predprob = alg.predict_proba(y_test)\n",
    "\n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"RMSE : %.4g\" % metrics.mean_squared_error(y_test, y_predictions))\n",
    "    #print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "\n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp = feat_imp.rename(lambda x: feature_names[int(x[1:len(x)])])\n",
    "    \n",
    "    feat_imp.plot(kind='barh', title='Feature Importances',figsize=(7, 16))\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ModelParamSearch(xgb, params, X_train, y_train):\n",
    "    search = GridSearchCV(estimator=xgb, param_grid=params, n_jobs=4, iid=False, cv=5)\n",
    "    search.fit(X_train, y_train)\n",
    "    print('\\ngrid_scores')\n",
    "    for score in search.grid_scores_:\n",
    "        print(score)\n",
    "    print('\\nbest_params')\n",
    "    print(search.best_params_)\n",
    "    print('\\nbest_score')\n",
    "    print(search.best_score_)\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "grid_scores\n",
      "mean: 0.63236, std: 0.02830, params: {'min_child_weight': 1, 'max_depth': 3}\n",
      "mean: 0.63283, std: 0.02815, params: {'min_child_weight': 2, 'max_depth': 3}\n",
      "mean: 0.63376, std: 0.03011, params: {'min_child_weight': 3, 'max_depth': 3}\n",
      "mean: 0.63394, std: 0.02881, params: {'min_child_weight': 4, 'max_depth': 3}\n",
      "mean: 0.63539, std: 0.03031, params: {'min_child_weight': 5, 'max_depth': 3}\n",
      "mean: 0.62739, std: 0.03049, params: {'min_child_weight': 1, 'max_depth': 4}\n",
      "mean: 0.62823, std: 0.02963, params: {'min_child_weight': 2, 'max_depth': 4}\n",
      "mean: 0.63081, std: 0.02900, params: {'min_child_weight': 3, 'max_depth': 4}\n",
      "mean: 0.63020, std: 0.03016, params: {'min_child_weight': 4, 'max_depth': 4}\n",
      "mean: 0.63164, std: 0.03068, params: {'min_child_weight': 5, 'max_depth': 4}\n",
      "mean: 0.62150, std: 0.02654, params: {'min_child_weight': 1, 'max_depth': 5}\n",
      "mean: 0.62029, std: 0.02536, params: {'min_child_weight': 2, 'max_depth': 5}\n",
      "mean: 0.61927, std: 0.02950, params: {'min_child_weight': 3, 'max_depth': 5}\n",
      "mean: 0.62026, std: 0.02719, params: {'min_child_weight': 4, 'max_depth': 5}\n",
      "mean: 0.62520, std: 0.03204, params: {'min_child_weight': 5, 'max_depth': 5}\n",
      "\n",
      "best_params\n",
      "{'min_child_weight': 5, 'max_depth': 3}\n",
      "\n",
      "best_score\n",
      "0.6353850783928838\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(max_depth=2, learning_rate=0.1, n_estimators=100,\\\n",
    "                             silent=False, objective='reg:linear', subsample=0.8,\\\n",
    "                             colsample_bytree=0.75, gamma=0, min_child_weight = 6,\\\n",
    "                             scale_pos_weight=1, seed=27)\n",
    "\n",
    "params1 = {\n",
    " 'max_depth':np.array(range(3, 6, 1)),\n",
    " 'min_child_weight':np.array(range(1,6, 1)),\n",
    "}\n",
    "\n",
    "search1 = ModelParamSearch(xgb_model, params1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "grid_scores\n",
      "mean: 0.62739, std: 0.02856, params: {'subsample': 0.5, 'colsample_bytree': 0.5}\n",
      "mean: 0.63243, std: 0.02958, params: {'subsample': 0.55, 'colsample_bytree': 0.5}\n",
      "mean: 0.63026, std: 0.02627, params: {'subsample': 0.6, 'colsample_bytree': 0.5}\n",
      "mean: 0.63127, std: 0.02678, params: {'subsample': 0.65, 'colsample_bytree': 0.5}\n",
      "mean: 0.63266, std: 0.02885, params: {'subsample': 0.7, 'colsample_bytree': 0.5}\n",
      "mean: 0.63125, std: 0.02808, params: {'subsample': 0.75, 'colsample_bytree': 0.5}\n",
      "mean: 0.63394, std: 0.02911, params: {'subsample': 0.8, 'colsample_bytree': 0.5}\n",
      "mean: 0.63513, std: 0.03226, params: {'subsample': 0.85, 'colsample_bytree': 0.5}\n",
      "mean: 0.62925, std: 0.03003, params: {'subsample': 0.5, 'colsample_bytree': 0.55}\n",
      "mean: 0.63195, std: 0.03029, params: {'subsample': 0.55, 'colsample_bytree': 0.55}\n",
      "mean: 0.62982, std: 0.02469, params: {'subsample': 0.6, 'colsample_bytree': 0.55}\n",
      "mean: 0.63106, std: 0.02893, params: {'subsample': 0.65, 'colsample_bytree': 0.55}\n",
      "mean: 0.63137, std: 0.02929, params: {'subsample': 0.7, 'colsample_bytree': 0.55}\n",
      "mean: 0.63071, std: 0.02869, params: {'subsample': 0.75, 'colsample_bytree': 0.55}\n",
      "mean: 0.63524, std: 0.02939, params: {'subsample': 0.8, 'colsample_bytree': 0.55}\n",
      "mean: 0.63639, std: 0.03070, params: {'subsample': 0.85, 'colsample_bytree': 0.55}\n",
      "mean: 0.63086, std: 0.02777, params: {'subsample': 0.5, 'colsample_bytree': 0.6}\n",
      "mean: 0.63171, std: 0.03013, params: {'subsample': 0.55, 'colsample_bytree': 0.6}\n",
      "mean: 0.63354, std: 0.02669, params: {'subsample': 0.6, 'colsample_bytree': 0.6}\n",
      "mean: 0.63223, std: 0.02586, params: {'subsample': 0.65, 'colsample_bytree': 0.6}\n",
      "mean: 0.63390, std: 0.02541, params: {'subsample': 0.7, 'colsample_bytree': 0.6}\n",
      "mean: 0.63351, std: 0.02839, params: {'subsample': 0.75, 'colsample_bytree': 0.6}\n",
      "mean: 0.63461, std: 0.02745, params: {'subsample': 0.8, 'colsample_bytree': 0.6}\n",
      "mean: 0.63602, std: 0.03052, params: {'subsample': 0.85, 'colsample_bytree': 0.6}\n",
      "mean: 0.63055, std: 0.02730, params: {'subsample': 0.5, 'colsample_bytree': 0.65}\n",
      "mean: 0.63022, std: 0.03027, params: {'subsample': 0.55, 'colsample_bytree': 0.65}\n",
      "mean: 0.63118, std: 0.03136, params: {'subsample': 0.6, 'colsample_bytree': 0.65}\n",
      "mean: 0.63165, std: 0.02617, params: {'subsample': 0.65, 'colsample_bytree': 0.65}\n",
      "mean: 0.63660, std: 0.02857, params: {'subsample': 0.7, 'colsample_bytree': 0.65}\n",
      "mean: 0.63528, std: 0.03019, params: {'subsample': 0.75, 'colsample_bytree': 0.65}\n",
      "mean: 0.63478, std: 0.03203, params: {'subsample': 0.8, 'colsample_bytree': 0.65}\n",
      "mean: 0.63536, std: 0.03167, params: {'subsample': 0.85, 'colsample_bytree': 0.65}\n",
      "mean: 0.62826, std: 0.02804, params: {'subsample': 0.5, 'colsample_bytree': 0.7}\n",
      "mean: 0.62948, std: 0.03097, params: {'subsample': 0.55, 'colsample_bytree': 0.7}\n",
      "mean: 0.63245, std: 0.02863, params: {'subsample': 0.6, 'colsample_bytree': 0.7}\n",
      "mean: 0.63315, std: 0.02899, params: {'subsample': 0.65, 'colsample_bytree': 0.7}\n",
      "mean: 0.63629, std: 0.03060, params: {'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "mean: 0.63473, std: 0.03026, params: {'subsample': 0.75, 'colsample_bytree': 0.7}\n",
      "mean: 0.63377, std: 0.03084, params: {'subsample': 0.8, 'colsample_bytree': 0.7}\n",
      "mean: 0.63684, std: 0.02862, params: {'subsample': 0.85, 'colsample_bytree': 0.7}\n",
      "mean: 0.62993, std: 0.03016, params: {'subsample': 0.5, 'colsample_bytree': 0.75}\n",
      "mean: 0.63180, std: 0.03049, params: {'subsample': 0.55, 'colsample_bytree': 0.75}\n",
      "mean: 0.63064, std: 0.02763, params: {'subsample': 0.6, 'colsample_bytree': 0.75}\n",
      "mean: 0.63338, std: 0.03011, params: {'subsample': 0.65, 'colsample_bytree': 0.75}\n",
      "mean: 0.63536, std: 0.02862, params: {'subsample': 0.7, 'colsample_bytree': 0.75}\n",
      "mean: 0.63559, std: 0.03182, params: {'subsample': 0.75, 'colsample_bytree': 0.75}\n",
      "mean: 0.63539, std: 0.03031, params: {'subsample': 0.8, 'colsample_bytree': 0.75}\n",
      "mean: 0.63904, std: 0.02920, params: {'subsample': 0.85, 'colsample_bytree': 0.75}\n",
      "mean: 0.63029, std: 0.02706, params: {'subsample': 0.5, 'colsample_bytree': 0.8}\n",
      "mean: 0.63196, std: 0.03079, params: {'subsample': 0.55, 'colsample_bytree': 0.8}\n",
      "mean: 0.63026, std: 0.02947, params: {'subsample': 0.6, 'colsample_bytree': 0.8}\n",
      "mean: 0.63337, std: 0.02979, params: {'subsample': 0.65, 'colsample_bytree': 0.8}\n",
      "mean: 0.63432, std: 0.02761, params: {'subsample': 0.7, 'colsample_bytree': 0.8}\n",
      "mean: 0.63120, std: 0.02800, params: {'subsample': 0.75, 'colsample_bytree': 0.8}\n",
      "mean: 0.63544, std: 0.03021, params: {'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "mean: 0.64027, std: 0.03093, params: {'subsample': 0.85, 'colsample_bytree': 0.8}\n",
      "mean: 0.62874, std: 0.02560, params: {'subsample': 0.5, 'colsample_bytree': 0.85}\n",
      "mean: 0.63126, std: 0.03100, params: {'subsample': 0.55, 'colsample_bytree': 0.85}\n",
      "mean: 0.62984, std: 0.02917, params: {'subsample': 0.6, 'colsample_bytree': 0.85}\n",
      "mean: 0.63322, std: 0.02956, params: {'subsample': 0.65, 'colsample_bytree': 0.85}\n",
      "mean: 0.63369, std: 0.02851, params: {'subsample': 0.7, 'colsample_bytree': 0.85}\n",
      "mean: 0.63542, std: 0.02871, params: {'subsample': 0.75, 'colsample_bytree': 0.85}\n",
      "mean: 0.63561, std: 0.02881, params: {'subsample': 0.8, 'colsample_bytree': 0.85}\n",
      "mean: 0.63938, std: 0.03109, params: {'subsample': 0.85, 'colsample_bytree': 0.85}\n",
      "\n",
      "best_params\n",
      "{'subsample': 0.85, 'colsample_bytree': 0.8}\n",
      "\n",
      "best_score\n",
      "0.6402731665511813\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100,\\\n",
    "                             silent=False, objective='reg:linear', subsample=0.8,\\\n",
    "                             colsample_bytree=0.75, gamma=0, min_child_weight = 5,\\\n",
    "                             scale_pos_weight=1, seed=27)\n",
    "\n",
    "params2 = {\n",
    " 'subsample':[i/100.0 for i in range(50, 90, 5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(50, 90, 5)]\n",
    "}\n",
    "\n",
    "search2 = ModelParamSearch(xgb_model, params2, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "grid_scores\n",
      "mean: -163.28237, std: 13.77925, params: {'learning_rate': 0.01, 'n_estimators': 100}\n",
      "mean: 0.63871, std: 0.02933, params: {'learning_rate': 0.01, 'n_estimators': 1000}\n",
      "mean: 0.62039, std: 0.03020, params: {'learning_rate': 0.01, 'n_estimators': 3000}\n",
      "mean: -2.15770, std: 0.13825, params: {'learning_rate': 0.03, 'n_estimators': 100}\n",
      "mean: 0.61970, std: 0.03006, params: {'learning_rate': 0.03, 'n_estimators': 1000}\n",
      "mean: 0.57926, std: 0.02264, params: {'learning_rate': 0.03, 'n_estimators': 3000}\n",
      "mean: 0.59756, std: 0.04145, params: {'learning_rate': 0.05, 'n_estimators': 100}\n",
      "mean: 0.60160, std: 0.02977, params: {'learning_rate': 0.05, 'n_estimators': 1000}\n",
      "mean: 0.55400, std: 0.02297, params: {'learning_rate': 0.05, 'n_estimators': 3000}\n",
      "\n",
      "best_params\n",
      "{'learning_rate': 0.01, 'n_estimators': 1000}\n",
      "\n",
      "best_score\n",
      "0.6387061470047611\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100,\\\n",
    "                             silent=False, objective='reg:linear', subsample=0.85,\\\n",
    "                             colsample_bytree=0.8, gamma=0, min_child_weight = 5,\\\n",
    "                             scale_pos_weight=1, seed=27)\n",
    "\n",
    "params3 = {\n",
    " 'learning_rate':[0.01, 0.03, 0.05],\n",
    " 'n_estimators':[100, 1000, 3000],\n",
    "}\n",
    "\n",
    "search3 = ModelParamSearch(xgb_model, params3, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "grid_scores\n",
      "mean: 0.64027, std: 0.03093, params: {'reg_alpha': 1e-05}\n",
      "mean: 0.63981, std: 0.03060, params: {'reg_alpha': 0.01}\n",
      "mean: 0.63928, std: 0.03134, params: {'reg_alpha': 0.1}\n",
      "mean: 0.63922, std: 0.02877, params: {'reg_alpha': 1}\n",
      "mean: -0.28445, std: 0.07618, params: {'reg_alpha': 100}\n",
      "\n",
      "best_params\n",
      "{'reg_alpha': 1e-05}\n",
      "\n",
      "best_score\n",
      "0.6402732273341691\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100,\\\n",
    "                             silent=False, objective='reg:linear', subsample=0.85,\\\n",
    "                             colsample_bytree=0.8, gamma=0, min_child_weight = 5,\\\n",
    "                             scale_pos_weight=1, seed=27)\n",
    "\n",
    "param4 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "search4 = ModelParamSearch(xgb_model, param4, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23594819550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, seed=0, missing=Nonemax_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', booster='gbtree', nthread=-1, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, seed=0, missing=None\n",
    "regr = xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100,\\\n",
    "                        silent=False, objective='reg:linear', subsample=0.85,\\\n",
    "                        colsample_bytree=0.7, gamma=0, min_child_weight = 5,\\\n",
    "                        scale_pos_weight=1, seed=27, reg_alpha=1e-05)\n",
    "\n",
    "plot_learning_curve(regr, 'gbdt', X_train, y_train, cv=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59088722316855979"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(X_train, y_train)\n",
    "regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr.fit(munged_train_df.values, label_df.values)\n",
    "\n",
    "# Run prediction on training set to get a rough idea of how well it does.\n",
    "# y_pred = regr.predict(munged_train_df.values)\n",
    "# y_test = label_df\n",
    "# print(\"XGBoost score on training set: \", rmse(y_test, y_pred))\n",
    "\n",
    "# Run prediction on the Kaggle test set.\n",
    "y_pred_xgb = regr.predict(munged_test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# I found this best alpha through cross-validation.\n",
    "best_alpha = 0.00099\n",
    "\n",
    "regr = Lasso(alpha=best_alpha, max_iter=50000)\n",
    "regr.fit(munged_train_df, label_df)\n",
    "\n",
    "# Run prediction on training set to get a rough idea of how well it does.\n",
    "y_pred = regr.predict(munged_train_df)\n",
    "y_test = label_df\n",
    "#print(\"Lasso score on training set: \", rmse(y_test, y_pred))\n",
    "\n",
    "# Run prediction on the Kaggle test set.\n",
    "y_pred_lasso = regr.predict(munged_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_pred = (y_pred_xgb + y_pred_lasso) / 2\n",
    "y_pred = y_pred_xgb\n",
    "y_pred = np.expm1(y_pred)\n",
    "\n",
    "pred_df = pd.DataFrame(y_pred, index=test_df.index, columns=[\"y\"])\n",
    "pred_df.to_csv('output.csv', header=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
